{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "constant-clock",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 1 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-02-28 22:29:18,500 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-28 22:29:18,586 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-28 22:29:18,710 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2021-02-28 22:29:19,476 INFO  Client: Requesting a new application from cluster with 5 NodeManagers\n",
      "2021-02-28 22:29:19,551 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.2-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-28 22:29:19,562 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-28 22:29:19,578 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50036 MB per container)\n",
      "2021-02-28 22:29:19,579 INFO  Client: Will allocate AM container, with 13252 MB memory including 1204 MB overhead\n",
      "2021-02-28 22:29:19,579 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-28 22:29:19,585 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-28 22:29:19,595 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-28 22:29:20,350 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-28 22:29:20,447 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-28 22:29:20,593 INFO  Client: Uploading resource file:/tmp/spark-4b003abf-fb85-4b5e-9a48-cf2a16191a84/__spark_conf__2862510338676359025.zip -> hdfs:/Projects/pits/Resources/.sparkStaging/application_1614531686983_0002/__spark_conf__.zip\n",
      "2021-02-28 22:29:21,050 INFO  SecurityManager: Changing view acls to: livy,pits__meb10179\n",
      "2021-02-28 22:29:21,051 INFO  SecurityManager: Changing modify acls to: livy,pits__meb10179\n",
      "2021-02-28 22:29:21,051 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-28 22:29:21,052 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-28 22:29:21,053 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, pits__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, pits__meb10179); groups with modify permissions: Set()\n",
      "2021-02-28 22:29:21,116 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-28 22:29:22,032 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-28 22:29:22,032 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-28 22:29:22,033 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-28 22:29:22,040 INFO  Client: Submitting application application_1614531686983_0002 to ResourceManager\n",
      "2021-02-28 22:29:23,029 INFO  Client: Deleted staging directory hdfs:/Projects/pits/Resources/.sparkStaging/application_1614531686983_0002\n",
      "2021-02-28 22:29:23,040 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-28 22:29:23,041 INFO  ShutdownHookManager: Deleting directory /tmp/spark-6b7a3a63-4465-4c0d-ac12-663e93416d45\n",
      "2021-02-28 22:29:23,078 INFO  ShutdownHookManager: Deleting directory /tmp/spark-4b003abf-fb85-4b5e-9a48-cf2a16191a84\n",
      "\n",
      "stderr: \n",
      "Exception in thread \"main\" java.lang.reflect.UndeclaredThrowableException\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1839)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:150)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:926)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:935)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request! Cannot allocate containers as requested resource is greater than maximum allowed allocation. Requested resource type=[vcores], Requested resource=<memory:13252, vCores:12>, maximum allowed allocation=<memory:50036, vCores:8>, please note that maximum allowed allocation is calculated by scheduler based on maximum resource of registered NodeManagers, which might be less than configured maximum allocation=<memory:64000, vCores:8, yarn.io/gpu: 9223372036854775807>\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.throwInvalidResourceException(SchedulerUtils.java:491)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.checkResourceRequestAgainstAvailableResource(SchedulerUtils.java:387)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:315)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndValidateRequest(SchedulerUtils.java:293)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.validateAndCreateResourceRequest(RMAppManager.java:590)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:424)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.submitApplication(RMAppManager.java:363)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.submitApplication(ClientRMService.java:659)\n",
      "\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:290)\n",
      "\tat org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:611)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:868)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:814)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1821)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2900)\n",
      "\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)\n",
      "\tat org.apache.hadoop.yarn.ipc.RPCUtil.instantiateYarnException(RPCUtil.java:75)\n",
      "\tat org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:116)\n",
      "\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.submitApplication(ApplicationClientProtocolPBClientImpl.java:304)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:430)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:171)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:163)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:101)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:367)\n",
      "\tat com.sun.proxy.$Proxy8.submitApplication(Unknown Source)\n",
      "\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.submitApplication(YarnClientImpl.java:291)\n",
      "\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:183)\n",
      "\tat org.apache.spark.deploy.yarn.Client.run(Client.scala:1134)\n",
      "\tat org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1526)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:851)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$3.run(SparkSubmit.scala:152)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$3.run(SparkSubmit.scala:150)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1821)\n",
      "\t... 6 more\n",
      "Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException): Invalid resource request! Cannot allocate containers as requested resource is greater than maximum allowed allocation. Requested resource type=[vcores], Requested resource=<memory:13252, vCores:12>, maximum allowed allocation=<memory:50036, vCores:8>, please note that maximum allowed allocation is calculated by scheduler based on maximum resource of registered NodeManagers, which might be less than configured maximum allocation=<memory:64000, vCores:8, yarn.io/gpu: 9223372036854775807>\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.throwInvalidResourceException(SchedulerUtils.java:491)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.checkResourceRequestAgainstAvailableResource(SchedulerUtils.java:387)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:315)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndValidateRequest(SchedulerUtils.java:293)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.validateAndCreateResourceRequest(RMAppManager.java:590)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:424)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.submitApplication(RMAppManager.java:363)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.submitApplication(ClientRMService.java:659)\n",
      "\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:290)\n",
      "\tat org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:611)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025).\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from pyspark.sql import DataFrame, Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, col\n",
    "from pyspark.sql import functions as F\n",
    "import sys\n",
    "from hops import hdfs as hdfs\n",
    "print(hdfs.project_path())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "female-folder",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 1 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-02-28 22:29:18,500 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-28 22:29:18,586 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-28 22:29:18,710 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2021-02-28 22:29:19,476 INFO  Client: Requesting a new application from cluster with 5 NodeManagers\n",
      "2021-02-28 22:29:19,551 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.2-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-28 22:29:19,562 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-28 22:29:19,578 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50036 MB per container)\n",
      "2021-02-28 22:29:19,579 INFO  Client: Will allocate AM container, with 13252 MB memory including 1204 MB overhead\n",
      "2021-02-28 22:29:19,579 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-28 22:29:19,585 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-28 22:29:19,595 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-28 22:29:20,350 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-28 22:29:20,447 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-28 22:29:20,593 INFO  Client: Uploading resource file:/tmp/spark-4b003abf-fb85-4b5e-9a48-cf2a16191a84/__spark_conf__2862510338676359025.zip -> hdfs:/Projects/pits/Resources/.sparkStaging/application_1614531686983_0002/__spark_conf__.zip\n",
      "2021-02-28 22:29:21,050 INFO  SecurityManager: Changing view acls to: livy,pits__meb10179\n",
      "2021-02-28 22:29:21,051 INFO  SecurityManager: Changing modify acls to: livy,pits__meb10179\n",
      "2021-02-28 22:29:21,051 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-28 22:29:21,052 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-28 22:29:21,053 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, pits__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, pits__meb10179); groups with modify permissions: Set()\n",
      "2021-02-28 22:29:21,116 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-28 22:29:22,032 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-28 22:29:22,032 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-28 22:29:22,033 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-28 22:29:22,040 INFO  Client: Submitting application application_1614531686983_0002 to ResourceManager\n",
      "2021-02-28 22:29:23,029 INFO  Client: Deleted staging directory hdfs:/Projects/pits/Resources/.sparkStaging/application_1614531686983_0002\n",
      "2021-02-28 22:29:23,040 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-28 22:29:23,041 INFO  ShutdownHookManager: Deleting directory /tmp/spark-6b7a3a63-4465-4c0d-ac12-663e93416d45\n",
      "2021-02-28 22:29:23,078 INFO  ShutdownHookManager: Deleting directory /tmp/spark-4b003abf-fb85-4b5e-9a48-cf2a16191a84\n",
      "\n",
      "stderr: \n",
      "Exception in thread \"main\" java.lang.reflect.UndeclaredThrowableException\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1839)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:150)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:926)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:935)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request! Cannot allocate containers as requested resource is greater than maximum allowed allocation. Requested resource type=[vcores], Requested resource=<memory:13252, vCores:12>, maximum allowed allocation=<memory:50036, vCores:8>, please note that maximum allowed allocation is calculated by scheduler based on maximum resource of registered NodeManagers, which might be less than configured maximum allocation=<memory:64000, vCores:8, yarn.io/gpu: 9223372036854775807>\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.throwInvalidResourceException(SchedulerUtils.java:491)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.checkResourceRequestAgainstAvailableResource(SchedulerUtils.java:387)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:315)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndValidateRequest(SchedulerUtils.java:293)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.validateAndCreateResourceRequest(RMAppManager.java:590)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:424)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.submitApplication(RMAppManager.java:363)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.submitApplication(ClientRMService.java:659)\n",
      "\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:290)\n",
      "\tat org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:611)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:868)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:814)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1821)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2900)\n",
      "\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)\n",
      "\tat org.apache.hadoop.yarn.ipc.RPCUtil.instantiateYarnException(RPCUtil.java:75)\n",
      "\tat org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:116)\n",
      "\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.submitApplication(ApplicationClientProtocolPBClientImpl.java:304)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:430)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:171)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:163)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:101)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:367)\n",
      "\tat com.sun.proxy.$Proxy8.submitApplication(Unknown Source)\n",
      "\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.submitApplication(YarnClientImpl.java:291)\n",
      "\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:183)\n",
      "\tat org.apache.spark.deploy.yarn.Client.run(Client.scala:1134)\n",
      "\tat org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1526)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:851)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$3.run(SparkSubmit.scala:152)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$3.run(SparkSubmit.scala:150)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1821)\n",
      "\t... 6 more\n",
      "Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException): Invalid resource request! Cannot allocate containers as requested resource is greater than maximum allowed allocation. Requested resource type=[vcores], Requested resource=<memory:13252, vCores:12>, maximum allowed allocation=<memory:50036, vCores:8>, please note that maximum allowed allocation is calculated by scheduler based on maximum resource of registered NodeManagers, which might be less than configured maximum allocation=<memory:64000, vCores:8, yarn.io/gpu: 9223372036854775807>\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.throwInvalidResourceException(SchedulerUtils.java:491)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.checkResourceRequestAgainstAvailableResource(SchedulerUtils.java:387)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:315)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndValidateRequest(SchedulerUtils.java:293)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.validateAndCreateResourceRequest(RMAppManager.java:590)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:424)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.submitApplication(RMAppManager.java:363)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.submitApplication(ClientRMService.java:659)\n",
      "\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:290)\n",
      "\tat org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:611)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025).\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "fg1_schema = StructType([\n",
    "  StructField(\"id\", IntegerType(), True),\n",
    "  StructField(\"ts\", IntegerType(), True),\n",
    "  StructField(\"f1\", StringType(), True)    \n",
    "])\n",
    "\n",
    "fg1=spark.read.csv(\"hdfs:///Projects/\" + hdfs.project_name() + \"/Resources/1000000-2000-2-out.csv\", header=True, schema=fg1_schema)\n",
    "fg1=fg1.sort(col(\"id\"),col(\"ts\"))\n",
    "fg1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "floral-blackjack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---+\n",
      "|id_2|ts_2| f2|\n",
      "+----+----+---+\n",
      "|   1|1020| f1|\n",
      "|   1|1040| f1|\n",
      "|   1|1060| f1|\n",
      "|   1|1080| f1|\n",
      "|   1|1100| f1|\n",
      "|   1|1120| f1|\n",
      "|   1|1140| f1|\n",
      "|   1|1160| f1|\n",
      "|   1|1180| f1|\n",
      "|   1|1200| f1|\n",
      "|   1|1220| f1|\n",
      "|   1|1240| f1|\n",
      "|   1|1260| f1|\n",
      "|   1|1280| f1|\n",
      "|   1|1300| f1|\n",
      "|   1|1320| f1|\n",
      "|   1|1340| f1|\n",
      "|   1|1360| f1|\n",
      "|   1|1380| f1|\n",
      "|   1|1400| f1|\n",
      "+----+----+---+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "fg2_schema = StructType([\n",
    "  StructField(\"id\", IntegerType(), True),\n",
    "  StructField(\"ts\", IntegerType(), True),\n",
    "  StructField(\"f2\", StringType(), True)    \n",
    "])\n",
    "\n",
    "fg2=spark.read.csv(\"hdfs:///Projects/\" + hdfs.project_name() + \"/Resources/1000000-2000-3-out.csv\", header=True, schema=fg2_schema)\n",
    "fg2=fg2.select([col(\"id\").alias(\"id_2\"), col(\"ts\").alias(\"ts_2\"), col(\"f2\")])\n",
    "fg2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "capable-thirty",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import lit, when, col, lag, rank\n",
    "\n",
    "win = Window.partitionBy([\"id\", \"ts\"]).orderBy(col('ts_2').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "contemporary-evening",
   "metadata": {},
   "outputs": [],
   "source": [
    "final=fg1.join(fg2, (fg1.id == fg2.id_2) & (fg1.ts >= fg2.ts_2)) \\\n",
    "    .withColumn(\"id_rank\", rank().over(win)) \\\n",
    "    .filter(col(\"id_rank\") == 1).drop(col(\"id_rank\")).drop(col(\"ts_2\")).drop(col(\"id_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "distant-bradley",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+---+\n",
      "| id|  ts| f1| f2|\n",
      "+---+----+---+---+\n",
      "|148|1020| f1| f1|\n",
      "|148|1040| f1| f1|\n",
      "|148|1060| f1| f1|\n",
      "|148|1080| f1| f1|\n",
      "|148|1100| f1| f1|\n",
      "|148|1120| f1| f1|\n",
      "|148|1140| f1| f1|\n",
      "|148|1160| f1| f1|\n",
      "|148|1180| f1| f1|\n",
      "|148|1200| f1| f1|\n",
      "|148|1220| f1| f1|\n",
      "|148|1240| f1| f1|\n",
      "|148|1260| f1| f1|\n",
      "|148|1280| f1| f1|\n",
      "|148|1300| f1| f1|\n",
      "|148|1320| f1| f1|\n",
      "|148|1340| f1| f1|\n",
      "|148|1360| f1| f1|\n",
      "|148|1380| f1| f1|\n",
      "|148|1400| f1| f1|\n",
      "+---+----+---+---+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-interpretation",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.write.parquet(hdfs.project_path() + \"Resources/joined.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-bumper",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
