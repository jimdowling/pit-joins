{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hollow-coast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 4 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-03-02 11:10:47,186 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-03-02 11:10:47,273 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-03-02 11:10:47,398 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2021-03-02 11:10:48,160 INFO  Client: Requesting a new application from cluster with 7 NodeManagers\n",
      "2021-03-02 11:10:48,240 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.2-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-03-02 11:10:48,251 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-03-02 11:10:48,268 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-03-02 11:10:48,941 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-03-02 11:10:48,942 INFO  ShutdownHookManager: Deleting directory /tmp/spark-cc1afd74-c9df-4209-b37e-440f0ac2d40c\n",
      "\n",
      "stderr: \n",
      "Exception in thread \"main\" java.lang.IllegalArgumentException: Required executor memory (25096), overhead (2509 MB), and PySpark memory (0 MB) is above the max threshold (24271 MB) of this cluster! Please check the values of 'yarn.scheduler.maximum-allocation-mb' and/or 'yarn.nodemanager.resource.memory-mb'.\n",
      "\tat org.apache.spark.deploy.yarn.Client.verifyClusterResources(Client.scala:345)\n",
      "\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:175)\n",
      "\tat org.apache.spark.deploy.yarn.Client.run(Client.scala:1134)\n",
      "\tat org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1526)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:851)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$3.run(SparkSubmit.scala:152)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$3.run(SparkSubmit.scala:150)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1821)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:150)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:926)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:935)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "\n",
      "YARN Diagnostics: \n",
      "No YARN application is found with tag livy-session-4-0wcr8jev in 120 seconds. This may be because 1) spark-submit fail to submit application to YARN; or 2) YARN cluster doesn't have enough resources to start the application in time. Please check Livy log and YARN log to know the details..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from pyspark.sql import DataFrame, Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, col\n",
    "from pyspark.sql import functions as F\n",
    "import sys\n",
    "from hops import hdfs as hdfs\n",
    "print(hdfs.project_path())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "surrounded-mapping",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 4 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-03-02 11:10:47,186 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-03-02 11:10:47,273 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-03-02 11:10:47,398 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2021-03-02 11:10:48,160 INFO  Client: Requesting a new application from cluster with 7 NodeManagers\n",
      "2021-03-02 11:10:48,240 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.2-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-03-02 11:10:48,251 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-03-02 11:10:48,268 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-03-02 11:10:48,941 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-03-02 11:10:48,942 INFO  ShutdownHookManager: Deleting directory /tmp/spark-cc1afd74-c9df-4209-b37e-440f0ac2d40c\n",
      "\n",
      "stderr: \n",
      "Exception in thread \"main\" java.lang.IllegalArgumentException: Required executor memory (25096), overhead (2509 MB), and PySpark memory (0 MB) is above the max threshold (24271 MB) of this cluster! Please check the values of 'yarn.scheduler.maximum-allocation-mb' and/or 'yarn.nodemanager.resource.memory-mb'.\n",
      "\tat org.apache.spark.deploy.yarn.Client.verifyClusterResources(Client.scala:345)\n",
      "\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:175)\n",
      "\tat org.apache.spark.deploy.yarn.Client.run(Client.scala:1134)\n",
      "\tat org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1526)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:851)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$3.run(SparkSubmit.scala:152)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$3.run(SparkSubmit.scala:150)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1821)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:150)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:926)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:935)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "\n",
      "YARN Diagnostics: \n",
      "No YARN application is found with tag livy-session-4-0wcr8jev in 120 seconds. This may be because 1) spark-submit fail to submit application to YARN; or 2) YARN cluster doesn't have enough resources to start the application in time. Please check Livy log and YARN log to know the details..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "fg1_schema = StructType([\n",
    "  StructField(\"id\", IntegerType(), True),\n",
    "  StructField(\"ts\", IntegerType(), True),\n",
    "  StructField(\"f1\", StringType(), True)    \n",
    "])\n",
    "\n",
    "fg1=spark.read.csv(\"hdfs:///Projects/\" + hdfs.project_name() + \"/Resources/1000000-2000-2-out.csv\", header=True, schema=fg1_schema)\n",
    "fg1=fg1.sort(col(\"id\"),col(\"ts\"))\n",
    "fg1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "military-question",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 4 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-03-02 11:10:47,186 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-03-02 11:10:47,273 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-03-02 11:10:47,398 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2021-03-02 11:10:48,160 INFO  Client: Requesting a new application from cluster with 7 NodeManagers\n",
      "2021-03-02 11:10:48,240 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.2-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-03-02 11:10:48,251 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-03-02 11:10:48,268 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-03-02 11:10:48,941 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-03-02 11:10:48,942 INFO  ShutdownHookManager: Deleting directory /tmp/spark-cc1afd74-c9df-4209-b37e-440f0ac2d40c\n",
      "\n",
      "stderr: \n",
      "Exception in thread \"main\" java.lang.IllegalArgumentException: Required executor memory (25096), overhead (2509 MB), and PySpark memory (0 MB) is above the max threshold (24271 MB) of this cluster! Please check the values of 'yarn.scheduler.maximum-allocation-mb' and/or 'yarn.nodemanager.resource.memory-mb'.\n",
      "\tat org.apache.spark.deploy.yarn.Client.verifyClusterResources(Client.scala:345)\n",
      "\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:175)\n",
      "\tat org.apache.spark.deploy.yarn.Client.run(Client.scala:1134)\n",
      "\tat org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1526)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:851)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$3.run(SparkSubmit.scala:152)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$3.run(SparkSubmit.scala:150)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1821)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:150)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:926)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:935)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "\n",
      "YARN Diagnostics: \n",
      "No YARN application is found with tag livy-session-4-0wcr8jev in 120 seconds. This may be because 1) spark-submit fail to submit application to YARN; or 2) YARN cluster doesn't have enough resources to start the application in time. Please check Livy log and YARN log to know the details..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "fg2_schema = StructType([\n",
    "  StructField(\"id\", IntegerType(), True),\n",
    "  StructField(\"ts\", IntegerType(), True),\n",
    "  StructField(\"f2\", StringType(), True)    \n",
    "])\n",
    "\n",
    "fg2=spark.read.csv(\"hdfs:///Projects/\" + hdfs.project_name() + \"/Resources/1000000-2000-3-out.csv\", header=True, schema=fg2_schema)\n",
    "fg2=fg2.select([col(\"id\").alias(\"id_2\"), col(\"ts\").alias(\"ts_2\"), col(\"f2\")])\n",
    "fg2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "progressive-manual",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 4 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-03-02 11:10:47,186 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-03-02 11:10:47,273 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-03-02 11:10:47,398 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2021-03-02 11:10:48,160 INFO  Client: Requesting a new application from cluster with 7 NodeManagers\n",
      "2021-03-02 11:10:48,240 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.2-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-03-02 11:10:48,251 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-03-02 11:10:48,268 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-03-02 11:10:48,941 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-03-02 11:10:48,942 INFO  ShutdownHookManager: Deleting directory /tmp/spark-cc1afd74-c9df-4209-b37e-440f0ac2d40c\n",
      "\n",
      "stderr: \n",
      "Exception in thread \"main\" java.lang.IllegalArgumentException: Required executor memory (25096), overhead (2509 MB), and PySpark memory (0 MB) is above the max threshold (24271 MB) of this cluster! Please check the values of 'yarn.scheduler.maximum-allocation-mb' and/or 'yarn.nodemanager.resource.memory-mb'.\n",
      "\tat org.apache.spark.deploy.yarn.Client.verifyClusterResources(Client.scala:345)\n",
      "\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:175)\n",
      "\tat org.apache.spark.deploy.yarn.Client.run(Client.scala:1134)\n",
      "\tat org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1526)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:851)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$3.run(SparkSubmit.scala:152)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$3.run(SparkSubmit.scala:150)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1821)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:150)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:926)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:935)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "\n",
      "YARN Diagnostics: \n",
      "No YARN application is found with tag livy-session-4-0wcr8jev in 120 seconds. This may be because 1) spark-submit fail to submit application to YARN; or 2) YARN cluster doesn't have enough resources to start the application in time. Please check Livy log and YARN log to know the details..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import lit, when, col, lag, rank\n",
    "\n",
    "win = Window.partitionBy([\"id\", \"ts\"]).orderBy(col('ts_2').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "synthetic-assault",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 4 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-03-02 11:10:47,186 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-03-02 11:10:47,273 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-03-02 11:10:47,398 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2021-03-02 11:10:48,160 INFO  Client: Requesting a new application from cluster with 7 NodeManagers\n",
      "2021-03-02 11:10:48,240 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.2-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-03-02 11:10:48,251 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-03-02 11:10:48,268 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-03-02 11:10:48,941 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-03-02 11:10:48,942 INFO  ShutdownHookManager: Deleting directory /tmp/spark-cc1afd74-c9df-4209-b37e-440f0ac2d40c\n",
      "\n",
      "stderr: \n",
      "Exception in thread \"main\" java.lang.IllegalArgumentException: Required executor memory (25096), overhead (2509 MB), and PySpark memory (0 MB) is above the max threshold (24271 MB) of this cluster! Please check the values of 'yarn.scheduler.maximum-allocation-mb' and/or 'yarn.nodemanager.resource.memory-mb'.\n",
      "\tat org.apache.spark.deploy.yarn.Client.verifyClusterResources(Client.scala:345)\n",
      "\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:175)\n",
      "\tat org.apache.spark.deploy.yarn.Client.run(Client.scala:1134)\n",
      "\tat org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1526)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:851)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$3.run(SparkSubmit.scala:152)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$3.run(SparkSubmit.scala:150)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1821)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:150)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:926)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:935)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "\n",
      "YARN Diagnostics: \n",
      "No YARN application is found with tag livy-session-4-0wcr8jev in 120 seconds. This may be because 1) spark-submit fail to submit application to YARN; or 2) YARN cluster doesn't have enough resources to start the application in time. Please check Livy log and YARN log to know the details..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "final=fg1.join(fg2, (fg1.id == fg2.id_2) & (fg1.ts >= fg2.ts_2)) \\\n",
    "    .withColumn(\"id_rank\", rank().over(win)) \\\n",
    "    .filter(col(\"id_rank\") == 1).drop(col(\"id_rank\")).drop(col(\"ts_2\")).drop(col(\"id_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "secure-master",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 4 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-03-02 11:10:47,186 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-03-02 11:10:47,273 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-03-02 11:10:47,398 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2021-03-02 11:10:48,160 INFO  Client: Requesting a new application from cluster with 7 NodeManagers\n",
      "2021-03-02 11:10:48,240 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.2-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-03-02 11:10:48,251 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-03-02 11:10:48,268 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-03-02 11:10:48,941 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-03-02 11:10:48,942 INFO  ShutdownHookManager: Deleting directory /tmp/spark-cc1afd74-c9df-4209-b37e-440f0ac2d40c\n",
      "\n",
      "stderr: \n",
      "Exception in thread \"main\" java.lang.IllegalArgumentException: Required executor memory (25096), overhead (2509 MB), and PySpark memory (0 MB) is above the max threshold (24271 MB) of this cluster! Please check the values of 'yarn.scheduler.maximum-allocation-mb' and/or 'yarn.nodemanager.resource.memory-mb'.\n",
      "\tat org.apache.spark.deploy.yarn.Client.verifyClusterResources(Client.scala:345)\n",
      "\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:175)\n",
      "\tat org.apache.spark.deploy.yarn.Client.run(Client.scala:1134)\n",
      "\tat org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1526)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:851)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$3.run(SparkSubmit.scala:152)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$3.run(SparkSubmit.scala:150)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1821)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:150)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:926)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:935)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "\n",
      "YARN Diagnostics: \n",
      "No YARN application is found with tag livy-session-4-0wcr8jev in 120 seconds. This may be because 1) spark-submit fail to submit application to YARN; or 2) YARN cluster doesn't have enough resources to start the application in time. Please check Livy log and YARN log to know the details..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "quarterly-artwork",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 4 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-03-02 11:10:47,186 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-03-02 11:10:47,273 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-03-02 11:10:47,398 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2021-03-02 11:10:48,160 INFO  Client: Requesting a new application from cluster with 7 NodeManagers\n",
      "2021-03-02 11:10:48,240 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.2-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-03-02 11:10:48,251 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-03-02 11:10:48,268 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-03-02 11:10:48,941 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-03-02 11:10:48,942 INFO  ShutdownHookManager: Deleting directory /tmp/spark-cc1afd74-c9df-4209-b37e-440f0ac2d40c\n",
      "\n",
      "stderr: \n",
      "Exception in thread \"main\" java.lang.IllegalArgumentException: Required executor memory (25096), overhead (2509 MB), and PySpark memory (0 MB) is above the max threshold (24271 MB) of this cluster! Please check the values of 'yarn.scheduler.maximum-allocation-mb' and/or 'yarn.nodemanager.resource.memory-mb'.\n",
      "\tat org.apache.spark.deploy.yarn.Client.verifyClusterResources(Client.scala:345)\n",
      "\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:175)\n",
      "\tat org.apache.spark.deploy.yarn.Client.run(Client.scala:1134)\n",
      "\tat org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1526)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:851)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$3.run(SparkSubmit.scala:152)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$3.run(SparkSubmit.scala:150)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1821)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:150)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:926)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:935)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "\n",
      "YARN Diagnostics: \n",
      "No YARN application is found with tag livy-session-4-0wcr8jev in 120 seconds. This may be because 1) spark-submit fail to submit application to YARN; or 2) YARN cluster doesn't have enough resources to start the application in time. Please check Livy log and YARN log to know the details..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "final.write.parquet(hdfs.project_path() + \"Resources/joined-10-workers.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-greece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
